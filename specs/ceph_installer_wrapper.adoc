// vim: tw=79

= Tendrl Ceph-installer Wrapper

Ceph-installer allows the administrator to provision and control the deployment
of Ceph clusters via a RESTFul API. Tendrl expected to make use of these APIs for provisioning the Ceph clusters.

== Problem description

Ceph-installer provides a set of Rest APIs which allows one to install and configure Ceph.Ceph-installer make use of ceph-ansible (ansible based deployment tool for provisioning ceph) internaly and exposes the Rest APIs that accepts and returns JSON responses with meaningful error codes and messages.  Tendrl is expected to perform Ceph deployment and planning to leverage the capabilities of ceph-installer by writing a wrapper for integration.

== Use Cases

* Creating the ceph cluster
  ** Install the packages
  ** Configure MONs and OSDs
* Expanding the ceph cluster
  ** Install Packages(if a new host is added)
  ** Configure MONs and OSDs
* Task management
  A task is created when an action on a remote node is triggered. Tasks are used to track the progress of the operation.

== Wrapper flows

A wrapper needs to be created which abstracts the capabilities provided by the ceph-installer.

=== Flows and atoms

* Create ceph cluster
  ** Package installation
  ** Configure MONs
  ** Configure OSDs
  ** Check task status

* Expand ceph cluster
  ** Add OSD
  ** Add a new node
  ** Check task status


== Data model impact

=== IP to UUID index for nodes

An index is required for resolving IP addresses to node UUIDs. Each IP address
for the each of the nodes must be mapped to the corresponding UUID in a global
index. This index is required for the API to be able to invoke jobs with a
UUID, when IP addresses are provided as node identification in the request.

=== Provisioning IP

In scenarios where the provisioning network is a separate network, the client
would be able to provide the IP address of any node which is to be used for
provisioning operations. The provisioning IP needs to be specifically stored
for every node and used as the IP address for ansible based provisioning
systems to communicate with the node on.



== Tendrl API impact

.API Endpoint
----
POST /1.0/CreateCluster
----


[source,json]
.Sample Response
----
Status: 202 Accepted
{ job_id: "5540ca6d-378c-4ce2-b632-46a43c5de835" }
----


=== Ceph specific parameters

==== Global parameters

fsid:: String, optional
ceph_conf_overrides:: Nested hash, optional. Each key corresponds to a section
in `ceph.conf` file. Currently supported are:
+
* global
* osd
* mon
+
The values for the section keys are hashes, each containing specific
`ceph.conf` parameters with their values.
public_network:: Network address with netmask, mandatory.
cluster_network:: Network address with netmask, mandatory.


==== Parameters for each node in the cluster

role:: ceph/mon or ceph/osd, mandatory
provisioning_ip:: IP address, optional, IP for the provisioning system to
communicate with the node on.

.Monitor node specific parameters
monitor_interface:: String, mandatory, mutually exclusive with `monitor_ip`.
monitor_ip:: IP address, mandatory, mutually exclusive with
`monitor_interface`.

.OSD node specific parameters
journal_size:: Integer (in megabytes), optional, defaults to 5GB.
journal_colocation:: Boolean, mandatory, See the validations section for more
information.

.Journal configuration on OSD nodes
storage_disks:: Array, mandatory, contains hashes describing device and journal
configuration. Hash keys:
+
* device
* journal
+
Both keys' values should be a single block device. The `journal_colocation`
value determines the validations applied. See the validations section for more
information.


=== Example requests

==== Default, UUID based, request

The default node identification is via UUID.


[source,json]
.Sample JSON
----
{
  "sds_type": "ceph",
  "sds_version": "10.2.5",
  "sds_parameters": {
    "name": "MyCluster",
    "fsid": "140cd3d5-58e4-4935-a954-d946ceff371d",
    "public_network": "192.168.128.0/24",
    "cluster_network": "192.168.220.0/24",
    "ceph_conf_overrides": {
      "global": {
        "osd_pool_default_pg_num": 128,
        "pool_default_pgp_num": 1
      }
    }
  },
  "node_configuration": {
    "3a95fd96-876d-439a-a64d-70332c069aaa": {
      "role": "ceph/mon",
      "monitor_interface": "eth0"
    },
    "3943fab1-9ed2-4eb6-8121-5a69499c4568": {
      "role": "ceph/osd",
      "journal_size": 5192,
      "journal_colocation": "false",
      "storage_disks": [
        {
          "device": "/dev/sda",
          "journal": "/dev/sdc"
        },
        {
          "dev_name": "/dev/sdb",
          "journal": "/dev/sdc"
        }
      ]
    },
    "b10e00e9-e444-41c2-9517-df2118b42731": {
      "role": "ceph/osd",
      "journal_colocation": "true",
      "storage_disks": [
        {
          "device": "/dev/sda"
        },
        {
          "dev_name": "/dev/sdb"
        }
      ]
    }
  }
}
----


==== IP based node identification

To use IP based node identification, a `node_identifier` parameter must be
sent with the value `ip` and the key for every node needs to be an ip address
on the node.


[source,json]
.Sample JSON
----
{
  "sds_type": "ceph",
  "sds_version": "10.2.5",
  "sds_parameters": {
    "name": "MyCluster",
    "fsid": "140cd3d5-58e4-4935-a954-d946ceff371d",
    "public_network": "192.168.128.0/24",
    "cluster_network": "192.168.220.0/24",
    "ceph_conf_overrides": {
      "global": {
        "osd_pool_default_pg_num": 128,
        "pool_default_pgp_num": 1
      }
    }
  },
  "node_identifier": "ip",
  "node_configuration": {
    "10.0.0.24": {
      "role": "ceph/mon",
      "monitor_ip": "192.168.128.22"
    },
    "10.0.0.29": {
      "role": "ceph/osd",
      "journal_size": 5192,
      "journal_colocation": "false",
      "storage_disks": [
        {
          "device": "/dev/sda",
          "journal": "/dev/sdc"
        },
        {
          "dev_name": "/dev/sdb",
          "journal": "/dev/sdc"
        }
      ]
    },
    "10.0.0.30": {
      "role": "ceph/osd",
      "journal_colocation": "true",
      "storage_disks": [
        {
          "device": "/dev/sda"
        },
        {
          "dev_name": "/dev/sdb"
        }
      ]
    }
  }
}
----


==== Provisioning IP specification per node

In scenarios where the provisioning network needs to be explicitly specified,
the `provisioning_ip` parameter can be added to the node. This works with both
UUID and IP based node identification.


[source,json]
.Provisioning IP specification with UUID based identification
----
{
  "sds_type": "ceph",
  "sds_version": "10.2.5",
  "sds_parameters": {
    "name": "MyCluster",
    "fsid": "140cd3d5-58e4-4935-a954-d946ceff371d",
    "public_network": "192.168.128.0/24",
    "cluster_network": "192.168.220.0/24",
    "ceph_conf_overrides": {
      "global": {
        "osd_pool_default_pg_num": 128,
        "pool_default_pgp_num": 1
      }
    }
  },
  "node_identifier": "uuid",
  "node_configuration": {
    "3a95fd96-876d-439a-a64d-70332c069aaa": {
      "role": "ceph/mon",
      "provisioning_ip": "10.0.0.24",
      "monitor_interface": "eth0"
    },
    "3943fab1-9ed2-4eb6-8121-5a69499c4568": {
      "role": "ceph/osd",
      "provisioning_ip": "10.0.0.29",
      "journal_size": 5192,
      "journal_colocation": "false",
      "storage_disks": [
        {
          "device": "/dev/sda",
          "journal": "/dev/sdc"
        },
        {
          "dev_name": "/dev/sdb",
          "journal": "/dev/sdc"
        }
      ]
    },
    "b10e00e9-e444-41c2-9517-df2118b42731": {
      "role": "ceph/osd",
      "provisioning_ip": "10.0.0.30",
      "journal_colocation": "true",
      "storage_disks": [
        {
          "device": "/dev/sda"
        },
        {
          "dev_name": "/dev/sdb"
        }
      ]
    }
  }
}
----


==== Provisioning IP specification per node with provisioning IP

Here, IP based node identifiers are used along with provisioning IPs for each
of the nodes. The IP used for the node identifier need not be the same as the
provisioning IP. The identifier IP could be any one of the IPs available on a
node.

[source,json]
.Provisioning IP specification with IP based identification
----
{
  "sds_type": "ceph",
  "sds_version": "10.2.5",
  "sds_parameters": {
    "name": "MyCluster",
    "fsid": "140cd3d5-58e4-4935-a954-d946ceff371d",
    "public_network": "192.168.128.0/24",
    "cluster_network": "192.168.220.0/24",
    "ceph_conf_overrides": {
      "global": {
        "osd_pool_default_pg_num": 128,
        "pool_default_pgp_num": 1
      }
    }
  },
  "node_identifier": "ip",
  "node_configuration": {
    "10.0.0.24": {
      "role": "ceph/mon",
      "provisioning_ip": "10.0.0.24",
      "monitor_interface": "eth0"
    },
    "10.0.0.29": {
      "role": "ceph/osd",
      "provisioning_ip": "10.0.0.29",
      "journal_size": 5192,
      "journal_colocation": "false",
      "storage_disks": [
        {
          "device": "/dev/sda",
          "journal": "/dev/sdc"
        },
        {
          "dev_name": "/dev/sdb",
          "journal": "/dev/sdc"
        }
      ]
    },
    "10.0.0.30": {
      "role": "ceph/osd",
      "provisioning_ip": "10.0.0.30",
      "journal_colocation": "true",
      "storage_disks": [
        {
          "device": "/dev/sda"
        },
        {
          "dev_name": "/dev/sdb"
        }
      ]
    }
  }
}
----


== Tendrl/node_agent impact

Since ceph installer does not support operating from a node inside the cluster,
the node agent running on the API node needs to be designated as a provisioner
for ceph cluster. This would be accomplished by adding a tag for
`provisioners/ceph`. The API would route jobs directly to this tag, with an
empty `node_ids` array in the job payload.


[source,json]
.Job structure with all the possible parameters
----
{
  "integration_id": "9a4b84e0-17b3-4543-af9f-e42000c52bfc",
  "run": "tendrl.flows.CreateCluster",
  "status": "new",
  "type": "node",
  "node_ids": [],
  "tags": ["provisioner/ceph"],
  "parameters": {
    "sds_type": "ceph",
    "sds_version": "10.2.5",
    "name": "MyCluster",
    "TendrlContext.integration_id": "9a4b84e0-17b3-4543-af9f-e42000c52bfc",
    "Node[]": [
      "3a95fd96-876d-439a-a64d-70332c069aaa",
      "3943fab1-9ed2-4eb6-8121-5a69499c4568",
      "b10e00e9-e444-41c2-9517-df2118b42731"
    ],
    "fsid": "140cd3d5-58e4-4935-a954-d946ceff371d",
    "public_network": "192.168.128.0/24",
    "cluster_network": "192.168.220.0/24",
    "ceph_conf_overrides": {
      "global": {
        "osd_pool_default_pg_num": 128,
        "pool_default_pgp_num": 1
      }
    },
    "node_configuration": {
      "3a95fd96-876d-439a-a64d-70332c069aaa": {
        "role": "ceph/mon",
        "provisioning_ip": "10.0.0.24",
        "monitor_interface": "eth0"
      },
      "3943fab1-9ed2-4eb6-8121-5a69499c4568": {
        "role": "ceph/osd",
        "provisioning_ip": "10.0.0.29",
        "journal_size": 5192,
        "journal_colocation": "false",
        "storage_disks": [
          {
            "device": "/dev/sda",
            "journal": "/dev/sdc"
          },
          {
            "dev_name": "/dev/sdb",
            "journal": "/dev/sdc"
          }
        ]
      },
      "b10e00e9-e444-41c2-9517-df2118b42731": {
        "role": "ceph/osd",
        "provisioning_ip": "10.0.0.30",
        "journal_colocation": "true",
        "storage_disks": [
          {
            "device": "/dev/sda"
          },
          {
            "dev_name": "/dev/sdb"
          }
        ]
      }
    }
  }
}
----


=== Validations

The validations to be carried out on these parameters are as follows, when the
role of a node is `ceph/osd`:

* `journal_size` is optional. Value should be in Megabytes.
* `journal_colocation` is optional and value should be a boolean.
* `storage_disks` is mandatory. If not provided, it's an error because the node
  cannot be part of the cluster as an osd node.
** It has to be an array.
** It has to contain hashes.
** Each hash has to contain a `device` key.
*** If `journal_colocation` is set to `true` _for the node_, the `journal` key
    cannot exist in the hash
*** If `journal_colocation` is set to `false` _for the node_, the `journal` key
    must contain a device
** Every device path specified in either the `device` or the `journal` key must
   be a valid block device on the node
** A device cannot be listed under both `device` and `journal` (across multiple
   hashes), per host
** A block device cannot be listed more than once as a `device`.

The following journal configurations are valid:

Dedicated journal::
Different values for `device` and `journal`.

Co-located journal::
Only the `device` value.

Multi-journal (implies dedicated journals)::
Same `journal` device specified for multiple devices.


== Implementation

* https://github.com/Tendrl/specifications/issues/48

=== Assignee(s)

Primary assignee:

nthomas-redhat

Other contributor(s):

=== Work Items

* https://github.com/Tendrl/ceph-integration/issues/106
* https://github.com/Tendrl/node-agent/issues/202

== Dependencies


None.


== Testing

End users can't directly test this feature, however the flows like ceph cluster creation and expansion will use this feature internally.


== Documentation impact

None.


== References

* http://docs.ceph.com/ceph-installer/docs/

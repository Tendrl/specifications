// vim: tw=79

= Alerting System Design

== Introduction

The alerting system is necessary for:

* Tracking problems on objects.
* Notifying the administrator of the change in state of the problem conditions.

The goal of the system is to ensure that an administrator has a near-real-time
view of the problems in the infrastructure that need to be addressed.

Alerting is an essential part of the health monitoring of any storage
infrastructure. It is critical that tendrl's alerting system is robust and
performant enough to serve this purpose.

The purpose of this specification is to document the alerting system design and
the corresponding terminology for future reference.


== Problem Description

The existing alert system has a few problems:

* Events above a certain priority are always written to etcd, drastically
  increasing the number of writes.
* The flow of information in the alerting system results in the backend alert
  condition being set from a generated notification for the condition.

This spec addresses these concerns and also clarifies the lifecycle of any
alert and the role of the various tendrl components in each step of the
lifecycle of an alert.


== Use Cases

* Setting alert conditions based on specifically tracked changes to the state
  of the managed storage systems and the tendrl infrastructure in general.
* Generating notifications for changes to the alert conditions for delivery to
  the administrator.
* Enabling a global view of all the alerts conditions in the tendrl
  infrastructure.

NOTE: Alert delivery to selected destinations, the configuration of such
delivery end-points etc. are outside the scope of this spec and are covered
under spec https://github.com/Tendrl/specifications/issues/190[#190].


== Design

=== Glossary of terms used

Event::
Event, in the context of tendrl, refers to a message received, intercepted or
generated by a tendrl component, generally due to a change tendrl should know
about.

Alert Condition::
Alert condition refers to a problem state of any object in the tendrl managed
infrastructure that is to be tracked by tendrl components for the purpose of
bringing it to the attention of the administrator.

Message::
Message is a generic term used to describe a well defined chunk of data that
tendrl core can understand. All events are formatted into messages before
submitting them to tendrl core. Based on the attribute values in a message, it
may be interpreted by tendrl core as an alert condition.

=== Lifecycle of an Alert

The lifecycle of an alert is consists of five stages:

. Event
. Message
. Transportation
. Alert Condition
. Notification

IMPORTANT: All of these stages are in the context of tendrl's alerting. The
term Alert tends to be used freely in many systems. However, it is important to
use the tendrl terminology when talking about an alert in the context of
tendrl. An alert for an external system could count as an Event in tendrl and
it is important to make this distinction explicit in every conversation.

==== Event

An Event could be generated in an external system, including the storage system
being managed by tendrl, or generated by a tendrl component. Some examples of
event generation are:

* Events from the gluster eventing framework that are generated due to a change
  in the gluster. These events are intercepted by the gluster-integration
  component.
* Tendrl's inventory sync generating an Event to indicate that a block device
  no longer exists on the system.
* Alert generated from a monitoring system such as grafana or collectd,
  intercepted by the tendrl component responsible for interacting with these
  systems.

==== Message

The generated Events cannot be used to set Alert Conditions in etcd without
them first being parsed and turned into Messages tendrl can understand.  There
are two places where the responsibility for this lies:

* In the integration components, which receive or intercept the Events from
  an external system. This includes the above example of gluster eventing
  framework events received by tendrl's gluster-integration.
* In the tendrl component which generates the Event. In this case, the
  Event should be generated with all the necessary metadata and attributes
  required for use as an Alert Condition. In short, a tendrl generated Event
  must always be a Message.

The outcome of the parsing phase is transformation of the event into a
Message. The Message is then handed to tendrl core transportation and setting
of the Alert Condition.

TODO: Add details about the attributes or link to an existing spec that details
them.

==== Transportation

A Message is handed to the node agent's message socket for transportation.
Node agent is responsible for creating a new Alert Condition or an update to
the state of an existing Alert Condition in etcd, under the global namespace
`/alerting`.

==== Alert Condition

Alert Conditions are set in etcd under the global `/alerting` namespace.  Alert
Conditions are point-in-time. They reflect the problem condition as it stands
at any given point in time. Depending upon the change in the problem condition
the state of the Alert Condition needs to be updated. This could also lead to
the deletion of the Alert Condition once the problem no longer exists.


==== Notification

The Notifier component is responsible for monitoring the Alert Conditions and
generating a Notification to be delivered to the administrator. The Notifier is
covered in spec https://github.com/Tendrl/specifications/issues/190[#190].


== Implementation

=== Message transportation and Alert Condition handling in the node agent

Node agent's message socket serves as a Message sink for all tendrl components
on a node. Messages put through the socket are all well formed Message objects
and are always logged via syslog. Currently, there is a logic in place which
also copies the Messages, based on their `priority` attribute, to etcd under
`/messages`. This was one of the causes of spurious writes to etcd. With the
addition of explicit Alert Condition related attributes to the Messages, it
would be possible to only take the Alert Condition related changes to etcd.

=== Alert Condition attributes in a Message and the corresponding behaviour

Three new attributes could be introduced in a Message that would enable the
node agent to update the Alert Conditions in etcd.

alert_condition_id::
`(UUID)` The existance of this attribute indicates that the Message sets a new or
modifies an existing Alert Condition. This attribute would be used to keep
track of the Alert Conditions in etcd. Different behaviours may automatically
be invoked by the node agent upon the existence of this value in a Message and
it's corresponding object in etcd:

* Create a new Alert Condition object in etcd if the object with this UUID is
  not present.
* Unconditionally overwrite the existing object in etcd with the attribute
  values present in the Message.

alert_condition_status::
`(String)` The status attribute could be used to set a custom status string for
an alert condition. An example of such a status string could be the strings
`up`, `down` and `degraded` associated with a gluster volume.

alert_condition_state::
`(Constant)` An Alert Condition must always map to one of the three states: `ok`,
`warn` and `fail`. It may be possible to use the `priority` and/or `severity`
attributes to be able to set this state, in lieu of explicit declaration in the
Message metadata.

alert_condition_unset::
`(Boolean)` This attribute has a slight overlap with the previously mentioned
`alert_condition_state`. Essentially, this attribute (or perhaps
`alert_condition_state` attribute's `ok` value) is to be used to indicate that
an existin Alert Condition no longer exists and has to be removed. The correct
use of this attribute would have to be coupled with the value of the
`alert_condition_status` attribute. The expected behaviour is that the Alert
Condition object is updated with all the attributes present in the Message, as
is normally done, in addition to setting a pre-defined TTL on the object so
that the Alert Condition object is removed after a few minutes. This behaviour
is to ensure that an administrator may be able to observe an Alert Condition
gracefully fade away from existance with the `alert_condition_status` possibly
informing the administrator as to the reason.

TODO: Update with additional details based on the `priority` and `severity`
attributes' usage.

== References

* Alert notifications:
  https://github.com/Tendrl/specifications/issues/190[#190].
* Enable core Gluster alerts from Tendrl:
  https://github.com/Tendrl/specifications/issues/180[#180].
* Grafana based alerts:
  https://github.com/Tendrl/specifications/issues/169[#169].


TODO: Add references to previous specs which cover message attributes, node
agent socket etc.

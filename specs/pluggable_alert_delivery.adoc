= Pluggable Alerts Delivery End-Points

== Introduction

The alerting module in tendrl needs to alert/notify

* State changes of tendrl managed entities
* Threshold breaches of utilizations of tendrl managed entities.

via mediums such as:

* SNMP-traps
* email

The different sources of alerts are:

* Performance monitoring threshold breach alerts generated by collectd.
* Alerts related to state changes of tendrl managed sds specific entities
  generated by sds-integration.
  ex: Tendrl/gluster-integration, Tendrl/ceph-integration

== Problem description

* The different tendrl supported alerting means need to be pluggable in the
  sense that adding any new alert notifying means should be an easy task and
  should not involve much changes in the core alerting framework.
* The alerting mechanism should have a way to set importance of alerts so that
  the alerts desingated as important will be notified to users even if the
  alerting module crashed and hence the alert could not be notified but will
  be alerted once the alerting module is started back.

== Use Cases

* Dynamic addition of new means of notifying an alert.

== Proposed change

* The alerts raised are persisted in '/alerts' directory of etcd before sending
  out notifications. The flow is as below:
** One socket is connected to for writes from collectd, read-only from the
node agent and read-write from the bridge.
** The generator(collectd or bridge) of the alert needs to put the alert on
this socket.
** Any alert on that socket is always taken to etcd by the node agent
** In addition to writing the generated alerts, the bridge also reads from the
socket. It then filters and chooses only the alerts it can act on
* The list of supported types of alerts will be maintained in etcd in the path
'/monitoring/alert_types'. This is done as follows:
** When node-agent/bridge(sources of alerts) is started, adds the types of alert
it handles to the etcd's directory mentioned above. This is implemented as
follows:
*** The alert handlers are implented as python classes that extend a common
base alert handler. And during the init of all defined handlers by a
common manager, the alert handlers will add the type of alerts they handle
to the list maintained by the module(bridge/node-agent). And once the init
of all handlers is complete, the module(bridge/node-agent) updates the list
to etcd. the structure of this list in etcd will look as briefed in section
'Impacted Modules=>Notifications/Monitoring impact=>API to list alert types'
** This list is exposed via an api by the alerting module which will be proxied
to by the tendrl-api.
* Alert meta-data:
** Each alert will carry additional attribute that carries its significance
which will be added by respective handler. The supported significance levels
can be 'HIGH', 'MEDIUM' and 'LOW'.This logic will be hardcoded in the codebase
that generates the alerts.
*** The 'LOW' significance level alerts will not be notified(by default but can
be overruled using a config field as mentioned below). But will feature in the
list of alerts in etcd.
*** The 'MEDIUM' significance level alerts will be attempted to be notified
*** The 'HIGH' significance level alerts will be re-attempted to be notified
if a previous attempt failed for some reason(crash of alertingc service).
The alerts marked as significance HIGH are deemed important and hence
their delivery is guranteed even in case of sudden crash of alerting module.
This is achieved by updating a field ('delivered': True) of the corresponding
notification end-point under the alert's 'delivery' tag in etcd to indicate
that the alert has been notified by a specific medium for an end-point.
When alerting module is started, it scans all alerts in '/alerts' directory and
sends out notifications for those that are marked significance 'HIGH' but are
not set with 'delivered':True under any of the delivery mechanisms under
'delivery' tag and by those media which have 'delivered': False. Also there'll
be a thread that frequently monitors this kind of 'High' severity alerts.
More details about the exact structure under "Data model impact" section. This
explicit alert notification reattempt thread will only be triggered for alerts
with 'delivery' tag whose presence indicates such an alert has been already
attempted.
** An alert will also have a field to ack it. Acking an alert can be done by
tendrl-user(lets say user is aware of a problem and has taken some action on it
and doesn't want to be shown the alert until the issue is completely rectified.
In such case user can ack such an alert using an api that tendrl-api exposes
and such an api will set 'acked': True and 'acked-by': <user_name> to indicate
user <user-name> acked the alert and doesn't want to see it in the list of
alerts) or occurence of negation of existing alert(ex: Lets say cpu utilization
of a node breached critical threshold and a corresponding alert is raised by
collectd and the alert is stored in etcd by the node-agent after a while when
the cpu utilization is back normal, previous alert corresponding to its
threshold breach needs to be acked as no more relevant. This is achieved by
setting 'acked' to True and 'ackedby' to tendrl which means tendrl has acked
this alert as no more relevant)
** In addition to the above, the alert contains details such as the id of the
node on which the alert was detected, the time stamp at which the alert was
observed, resource for which the alert was generated, the type of alert,
severity of alert etc... as described in section "Data model impact".
* The Tendrl/alerting module watches for new alerts in etcd's /alerts directory
and sends out notifications to configured destinations via configured means
ex: mails or sms or snmp traps.
The different notification medium will be hooked into tendrl in a pluggable
manner as follows:
** Handlers will be defined in alerting module for dispatching observed alerts
to configured destinations through respective means.
ex: mail handler for sending out mail and snmp handler for snmp-trap
*** A new handler can be hooked into the tendrl alerting system by adding it to
a designated directory and with a name prefixed with notification medium name.
ex: email_handler for mail notification in the directory
alerting/tendrl/alerting/notification/
*** During the init of notification handlers, plugin framework after loading
plugins will upload the supported notification medium list to etcd in the path
'/alerts/notification_medium'.
*** The alerting module will expose apis for managing configurations and
subscriptions for each of the notification medium which will be proxied to by
tendrl-api.
The way the api works is as follows:
**** The configuration if applicable will link to the tendrl maintained users
by their id.ex: User->email config linking is possible but not
User->snmp configuration.
**** The configuration for each of these handlers will be stored in a
respective directory in etcd.
ex: Email configuration will be maintained in a etcd's directory
(ex: /notification_medium/email/config). Likewise, snmp configuration will be
maintained in etcd's '/notification_medium/snmp/config' directory.
**** The handler takes the responsibility of performing validation of the
configuration. Some of the sample validations include:
***** Presence of configuration for admin user before any email notification is
dispatched to act as source(from) of all mail notifications.
***** Presence of all mandatory fields in config mandated by the respective
handler.
**** Notification meta-data:
***** Each configuration will include a field 'alert_subscriptions' which can
hold possible values {empty}* to receive all alerts of the clusters in list of
cluster ids captured under tag 'clusters'.The possible value for tag 'clusters'
can be list of cluster ids or {empty}* to indicate all clusters and even nodes not
belonging to any clusters.
***** A field 'notify_low_sig_alert' which is by default false but the config
can be edited to receive even the alerts marked as low significant.

----
  Note:
    * The different alerts need to be yet classified on various importance
      levels.
      And this is my current opinion:
      **  Any threshold breach corresponding notification is a 'HIGH'
          significance alerts
    * The complete list of alerts and their importance/significance level needs
      to be decided.Following are some of the github issues to capture the list
      ** https://github.com/Tendrl/documentation/issues/44
      ** https://github.com/Tendrl/documentation/issues/45
      ** https://github.com/Tendrl/documentation/issues/46
----

=== Alternatives

To be explored.

=== Data model impact

The structure of alert can be the following:

----
{
  'alert-id': <unique tendrl generated id>,
  'node-id': <id of node on which alert was detected>,
  'time-stamp': <time stamp of alert>,
  'resource': <the name of resource for which alert has been raised>,
  'current-value': <the current observed value status/utilization as applies>
  'tags': <custom alert specific info>,
  'type': <the type of alert percent-used/status of resource>,
  'severity': <severity of alert>,
  'significance': <the severity of importance of notifying the alert>,
  'ackedby': <indicate who acked the alert>,
  'acked': <boolean to indicate if the alert is acked>
}
----

ex:
For a performance monitoring related alert:

----
{
  alert_id : '6405962e-bc46-11e6-a4a6-cec0c932ce01',
  node_id: '5205962e-bc46-11e6-a4a6-cec0c932cz01',
  time_stamp: '1481046935.536',
  'resource': 'memory',
  'CurrentValue': '7.176942e+00',
  tags: {
    'WarningMax': '1.000000e+00',
    'FailureMax': '2.000000e+00',
  },
  'Type': 'percent-used',
  'severity': 'Critical',
  'ackedby': '',
  'acked': False,
  'significance': 'HIGH',
}
----

and for a status based alert:

----
{
  alert_id : '6405962e-bc46-11e6-a4a6-cec0c932ce01',
  node_id: '5205962e-bc46-11e6-a4a6-cec0c932cz01',
  time_stamp: '1481046935.536',
  'resource': 'cluster',
  'CurrentValue': 'Down',
  tags: {
    'Tendrl_context.cluster_id' : '6406062e-be46-11e6-a4a6-cec0c932ce01',
    'Tendrl_context.sds_name': 'gluster-integration',
    'Tendrl_context.sds_version': 0.1,
  },
  'Type': 'status',
  'severity': 'Critical',
  'ackedby': '',
  'acked': False,
  'significance': 'HIGH',
}
----

The notification configuration will contain the folowing fields in addition to
the fields specific to the particular notification medium:

----
  'alert_subscriptions': {
      'detail': '* for all alerts. Or list of required types of alerts'
                'Not required for admin user(is_admin: True)',
      'type': "String(for '*') or List of alert types"
  },
  'clusters': {
    'detail': '* to receive all alerts or list of clusters'
              'Not required for admin user(is_admin: True)',
    'type': "String(for '*') or List of cluster-ids"
  }
----

The severity levels can be 'Critical', 'Info' or 'Warning'.

Once a new alert is detected from etcd, it'll be passed to a queue shared by
all notification handlers. The notification handlers detects from the configs
if it needs to dispatch the alert to any destination based on configs it
maintains and if yes, it adds some additional fields like:

----
'delivery': {
  'smtp': {
    'endpoint': 'foo@bar.com',
    'delivered': False,
    'attempt_count': 2,
    'last_attempted': <time_stamp>,
  }, 'snmp': {
    'endpoint': 'foobar',
    'delivered': False,
    'attempt_count': 1,
    'last_attempted': <time_stamp>,
  }
}

where delivery contains fields like
* 'delivered' to indicate if the delivery attempt succeded.
* 'last_attempted' to indicate time stamp of last delivery attempted
* 'attempt_count' to indicate the number of retries attempted for delivery

Note:
* The attempt count when crosses a configurable threshold, stops reattempts.
  ** This configuration is available only at a global level.
* Once the 'delivery' tag is present in an alert, further updates can only
  update 'last_attempted', increment 'attempt_count' and update 'delivered'
  but not modify/add any other field.
* These are states maintained as part of an alert in etcd
----

=== Impacted Modules:

==== Tendrl API impact:

The tendrl api needs to proxy to apis exposed by Tendrl/alerting as mentioned
in section below(Notifications/Monitoring impact)

==== Notifications/Monitoring impact:

Tendrl/alerting needs to implement flows for apis as described in section
"Tendrl API impact".

The flow definition for the above will look like:

----
# flake8: noqa
data = """---
namespace.tendrl.alerting:
  objects:
    Alert:
      attrs:
        alert-id:
          type: String
        node-id:
          type: String
        time-stamp:
          type: String
        resource:
          type: String
        current-value:
          type: String
        tags:
          type: json
        type:
          type: String
        severity:
          type: String
        significance:
          type: String
        ackedby:
          type: String
        acked:
          type: Boolean
      enabled: true
      value: alerts/$Alert.alert_id
      list: alerts/
        filter_criteria:
          type: json
    NotificationMedia:
      attrs:
        name:
          type: String
        list: alerts/notification_medium/
tendrl_schema_version: 0.3
"""
----

* This adds the following apis:
  ** Api to get list of currently supported means of notification.

----
GET /alerting/supported_notification_medium

Sample Response:

Status: 200 OK
{
  notif_medium: [email, snmp]
}
----

  ** Api to get list of alerts with various filtering options such as based on
     time, acked/not acked, alert type, severity, resource and significance.

----
GET /alerts/severity=CRITICAL

Sample Response:

Status: 200 OK
{
  'resource': u'swap',
  'severity': u'CRITICAL',
  'tags': {
    'message': u'Host dhcp43-30.lab.eng.blr.redhat.com,plugin swap type percent (instance used): Data source "value" is currently 2.399964. That is above failure threshold of 2.000000.\n',
    'warning_max': u'1.000000e+00',
    'failure_max': u'2.000000e+00'
  },
  'pid': '21688',
  'source': 'collectd',
  'host': u'dhcp43-30.lab.eng.blr.redhat.com',
  'current_value': u'2.399964e+00',
  'time_stamp': u'1481345075.096',
  'type': u'percent'
}
----

  ** API to post configuration
----
Note: API format to be worked through in separate specs as they are very much
      handler specific and cannot be generalised. Separate specs will be raised
      for each handler detailing out its specifics.
----

  ** API to list alert types

----
GET /monitoring/alert_types

Sample Response:

Status: 200 OK
{
  alert_types: [
    'performance_monitoring' : {
      cpu': {
        'threshold': [
          'Warning',
          'Critical',
          'Ok'
        ],
      },
      'swap': {
        'threshold': [
          'Warning',
          'Critical',
          'Ok'
        ],
      },
      'memory': {
        'threshold': [
          'Warning',
          'Critical',
          'Ok'
        ],
      },....
    },
    'ceph-integration': {
      'osd_status': {
        'In',
        'Out',
        'Paused',
        .
        .
        .
      },
      'cluster_status': {
        .
        .
        .
      }
    }
  ]
}

----

==== Tendrl/common impact:

None

==== Tendrl/node_agent impact:

None

==== Sds integration impact:

None

=== Security impact:

None

=== Other end user impact

None

=== Performance impact

None

=== Other deployer impact

None


=== Developer impact

== Implementation

=== Assignee(s)

Primary assignee:

  * Changes in alerting module : Anmol Babu

=== Work Items:

* https://github.com/Tendrl/alerting/issues/10
* https://github.com/Tendrl/alerting/issues/11
  ** https://github.com/Tendrl/alerting/issues/12
  ** https://github.com/Tendrl/alerting/issues/13
* https://github.com/Tendrl/alerting/issues/14

== Dependencies:

* User management in tendrl.

== Documentation impact

As described in section "Tendrl API impact" new apis will be added.

== Testing

This spec introduces an api to list available means of alert notification which
needs to be tested.

== References

* Comments on https://github.com/Tendrl/alerting/pull/1
* https://github.com/Tendrl/documentation/issues/44
* https://github.com/Tendrl/documentation/issues/45
* https://github.com/Tendrl/documentation/issues/46

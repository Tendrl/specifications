= Pluggable Alerts Delivery End-Points

== Introduction

The alerting module in tendrl needs to alert/notify

* State changes of tendrl managed entities
* Threshold breaches of utilizations of tendrl managed entities.

via mediums such as:

* SNMP-traps
* email

The different sources of alerts are:

* Performance monitoring threshold breach alerts generated by collectd.
* Alerts related to state changes of tendrl managed sds specific entities
  generated by sds-integration.
  ex: Tendrl/gluster-integration, Tendrl/ceph-integration

== Problem description

* The different tendrl supported alerting means need to be pluggable in the
  sense that adding any new alert notifying means should be an easy task and
  should not involve much changes in the core alerting framework.
* The alerting mechanism should have a way to set importance of alerts so that
  the alerts desingated as important will be notified to users even if the
  alerting module crashed and hence the alert could not be notified but will
  be alerted once the alerting module is started back.

== Use Cases

* Dynamic addition of new means of notifying an alert.

== Proposed change

* The alerts raised are persisted in '/alerts' directory of etcd before sending
  out notifications. The flow is as below:
** One socket is connected to for writes from collectd, read-only from the
node agent and read-write from the bridge.
** The generator(collectd or bridge) of the alert needs to put the alert on
this socket.
** Any alert on that socket is always taken to etcd by the node agent
** In addition to writing the generated alerts, the bridge also reads from the
socket. It then filters and chooses only the alerts it can act on
* Alert meta-data:
** Each alert will carry additional attribute that carries its significance
which will be added by respective handler. The supported significance levels
can be 'HIGH', 'MEDIUM' and 'LOW'.This logic will be hardcoded in the codebase
that generates the alerts.
*** The 'LOW' significance level alerts will not be notified(by default but can
be overruled using a config field as mentioned below). But will feature in the
list of alerts in etcd.
*** The 'MEDIUM' significance level alerts will be attempted to be notified
*** The 'HIGH' significance level alerts will be re-attempted to be notified
if a previous attempt failed for some reason(crash of alertingc service).
The alerts marked as significance HIGH are deemed important and hence
their delivery is guranteed even in case of sudden crash of alerting module.
This is achieved by updating a field ('delivered': True) of the corresponding
notification end-point under the alert's 'delivery' tag in etcd to indicate
that the alert has been notified by a specific medium for an end-point.
When alerting module is started, it scans all alerts in '/alerts' directory and
sends out notifications for those that are marked significance 'HIGH' but are
not set with 'delivered':True under any of the delivery mechanisms under
'delivery' tag and by those media which have 'delivered': False. Also there'll
be a thread that frequently monitors this kind of 'High' severity alerts.
More details about the exact structure under "Data model impact" section. This
explicit alert notification reattempt thread will only be triggered for alerts
with 'delivery' tag whose presence indicates such an alert has been already
attempted.
** An alert will also have a field to ack it. Acking an alert can be done by
tendrl-user(lets say user is aware of a problem and has taken some action on it
and doesn't want to be shown the alert until the issue is completely rectified.
In such case user can ack such an alert using an api that tendrl-api exposes
and such an api will set 'acked': True and 'acked-by': <user_name> to indicate
user <user-name> acked the alert and doesn't want to see it in the list of
alerts) or occurence of negation of existing alert(ex: Lets say cpu utilization
of a node breached critical threshold and a corresponding alert is raised by
collectd and the alert is stored in etcd by the node-agent after a while when
the cpu utilization is back normal, previous alert corresponding to its
threshold breach needs to be acked as no more relevant. This is achieved by
setting 'acked' to True and 'ackedby' to tendrl which means tendrl has acked
this alert as no more relevant)
** In addition to the above, the alert contains details such as the id of the
node on which the alert was detected, the time stamp at which the alert was
observed, resource for which the alert was generated, the type of alert,
severity of alert etc... as described in section "Data model impact".
* The Tendrl/alerting module watches for new alerts in etcd's /alerts directory
and sends out notifications to configured destinations via configured means
ex: mails or sms or snmp traps.
The different notification medium will be hooked into tendrl in a pluggable
manner as follows:
** Handlers will be defined in alerting module for dispatching observed alerts
to configured destinations through respective means.
ex: mail handler for sending out mail and snmp handler for snmp-trap
*** A new handler can be hooked into the tendrl alerting system by adding it to
a designated directory and with a name prefixed with notification medium name.
ex: email_handler for mail notification in the directory
alerting/tendrl/alerting/notification/
*** Whenever a new handler is added, the first thing the handler is supposed to
do is add itself as a means to list of supported notification mechanisms in
etcd in '/notification_medium' directory.
*** The alerting module will expose apis for managing configurations and
subscriptions for each of the notification medium which will be proxied to by
tendrl-api.
The way the api works is as follows:
**** The configuration if applicable will link to the tendrl maintained users
by their id.ex: User->email config linking is possible but not
User->snmp configuration.
**** The configuration for each of these handlers will be stored in a
respective directory in etcd.
ex: Email configuration will be maintained in a etcd's directory
(ex: /notification_medium/email/config). Likewise, snmp configuration will be
maintained in etcd's '/notification_medium/snmp/config' directory.
**** The handler takes the responsibility of performing validation of the
configuration. Some of the sample validations include:
***** Presence of configuration for admin user before any email notification is
dispatched to act as source(from) of all mail notifications.
***** Presence of all mandatory fields in config mandated by the respective
handler.
**** Notification meta-data:
***** Each configuration will include a field 'entitlements' which can
hold possible values '*' to receive all notifications for the configuration
or list of clusters which means destination which configuration corresponds to
likes to receive all notifications for the clusters in the list.
***** A field 'notify_low_sig_alert' which is by default false but the config
can be edited to receive even the alerts marked as low significant.

----
  Note:
    * The different alerts need to be yet classified on various importance
      levels.
      And this is my current opinion:
      **  Any threshold breach corresponding notification is a 'HIGH'
          significance alerts
    * The complete list of alerts and their importance/significance level needs
      to be decided.Following are some of the github issues to capture the list
      ** https://github.com/Tendrl/documentation/issues/44
      ** https://github.com/Tendrl/documentation/issues/45
      ** https://github.com/Tendrl/documentation/issues/46
----

=== Alternatives

To be explored.

=== Data model impact

The structure of alert can be the following:

----
{
  'alert-id': <unique tendrl generated id>,
  'node-id': <id of node on which alert was detected>,
  'time-stamp': <time stamp of alert>,
  'resource': <the name of resource for which alert has been raised>,
  'current-value': <the current observed value status/utilization as applies>
  'tags': <custom alert specific info>,
  'type': <the type of alert percent-used/status of resource>,
  'severity': <severity of alert>,
  'significance': <the severity of importance of notifying the alert>,
  'ackedby': <indicate who acked the alert>,
  'acked': <boolean to indicate if the alert is acked>
}
----

ex:
For a performance monitoring related alert:

----
{
  alert_id : '6405962e-bc46-11e6-a4a6-cec0c932ce01',
  node_id: '5205962e-bc46-11e6-a4a6-cec0c932cz01',
  time_stamp: '1481046935.536',
  'resource': 'memory',
  'CurrentValue': '7.176942e+00',
  tags: {
    'WarningMax': '1.000000e+00',
    'FailureMax': '2.000000e+00',
  },
  'Type': 'percent-used',
  'severity': 'Critical',
  'ackedby': '',
  'acked': False,
  'significance': 'HIGH',
}
----

and for a status based alert:

----
{
  alert_id : '6405962e-bc46-11e6-a4a6-cec0c932ce01',
  node_id: '5205962e-bc46-11e6-a4a6-cec0c932cz01',
  time_stamp: '1481046935.536',
  'resource': 'cluster',
  'CurrentValue': 'Down',
  tags: {
    'Tendrl_context.cluster_id' : '6406062e-be46-11e6-a4a6-cec0c932ce01',
    'Tendrl_context.sds_name': 'gluster-integration',
    'Tendrl_context.sds_version': 0.1,
  },
  'Type': 'status',
  'severity': 'Critical',
  'ackedby': '',
  'acked': False,
  'significance': 'HIGH',
}
----

The severity levels can be 'Critical', 'Info' or 'Warning'.

Once a new alert is detected from etcd, it'll be passed to a queue shared by
all notification handlers. The notification handlers detects from the configs
if it needs to dispatch the alert to any destination based on configs it
maintains and if yes, it adds some additional fields like:

----
'delivery': {
  'smtp': {
    'endpoint': 'foo@bar.com',
    'delivered': False,
    'attempt_count': 2,
    'last_attempted': <time_stamp>,
  }, 'snmp': {
    'endpoint': 'foobar',
    'delivered': False,
    'attempt_count': 1,
    'last_attempted': <time_stamp>,
  }
}

where delivery contains fields like
* 'delivered' to indicate if the delivery attempt succeded.
* 'last_attempted' to indicate time stamp of last delivery attempted
* 'attempt_count' to indicate the number of retries attempted for delivery

Note:
* The attempt count when crosses a configurable threshold, stops reattempts.
  ** This configuration is available only at a global level.
* Once the 'delivery' tag is present in an alert, further updates can only
  update 'last_attempted', increment 'attempt_count' and update 'delivered'
  but not modify/add any other field.
* These are states maintained as part of an alert in etcd
----

=== Impacted Modules:

==== Tendrl API impact:

The tendrl api needs to proxy to apis exposed by Tendrl/alerting as mentioned
in section below(Notifications/Monitoring impact)

==== Notifications/Monitoring impact:

Tendrl/alerting needs to implement flows for apis as described in section
"Tendrl API impact".

The flow definition for the above will look like:

----
# flake8: noqa
data = """---
namespace.tendrl.alerting:
  objects:
    Alert:
      attrs:
        alert-id:
          type: String
        node-id:
          type: String
        time-stamp:
          type: String
        resource:
          type: String
        current-value:
          type: String
        tags:
          type: json
        type:
          type: String
        severity:
          type: String
        significance:
          type: String
        ackedby:
          type: String
        acked:
          type: Boolean
      enabled: true
      value: alerts/$Alert.alert_id
      list: alerts/
        filter_criteria:
          type: json
    NotificationMedia:
      attrs:
        name:
          type: String
        list: alerts/notification_medium/
tendrl_schema_version: 0.3
"""
----

* This adds the following apis:
  ** Api to get list of currently supported means of notification.

----
GET /alerting/supported_notification_medium

Sample Response:

Status: 200 OK
{
  notif_medium: [email, snmp]
}
----

  ** Api to get list of alerts with various filtering options such as based on
     time, acked/not acked, alert type, severity, resource and significance.

----
GET /alerts/severity=CRITICAL

Sample Response:

Status: 200 OK
{
  'resource': u'swap',
  'severity': u'CRITICAL',
  'tags': {
    'message': u'Host dhcp43-30.lab.eng.blr.redhat.com,plugin swap type percent (instance used): Data source "value" is currently 2.399964. That is above failure threshold of 2.000000.\n',
    'warning_max': u'1.000000e+00',
    'failure_max': u'2.000000e+00'
  },
  'pid': '21688',
  'source': 'collectd',
  'host': u'dhcp43-30.lab.eng.blr.redhat.com',
  'current_value': u'2.399964e+00',
  'time_stamp': u'1481345075.096',
  'type': u'percent'
}
----

  ** API to post configuration
----
Note: API format to be worked through
----

==== Tendrl/common impact:

None

==== Tendrl/node_agent impact:

None

==== Sds integration impact:

None

=== Security impact:

None

=== Other end user impact

None

=== Performance impact

None

=== Other deployer impact

None


=== Developer impact

== Implementation

=== Assignee(s)

Primary assignee:

  * Changes in alerting module : Anmol Babu

=== Work Items:

* https://github.com/Tendrl/alerting/issues/10
* https://github.com/Tendrl/alerting/issues/11
  ** https://github.com/Tendrl/alerting/issues/12
  ** https://github.com/Tendrl/alerting/issues/13
* https://github.com/Tendrl/alerting/issues/14

== Dependencies:

* User management in tendrl.

== Documentation impact

As described in section "Tendrl API impact" new apis will be added.

== Testing

This spec introduces an api to list available means of alert notification which
needs to be tested.

== References

* Comments on https://github.com/Tendrl/alerting/pull/1
* https://github.com/Tendrl/documentation/issues/44
* https://github.com/Tendrl/documentation/issues/45
* https://github.com/Tendrl/documentation/issues/46
